{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GWAS 1: Window Size Analysis SD = 0.5\n",
    "Author: Sophie Sigfstead \n",
    "\n",
    "Purpose: This is an in-depth analysis of loci size and how it affects the results of our method for GWAS 1 (Depression).\n",
    "Specifically, my goal was to test various window sizes and see how this affected the accuracy of both our method (i.e., using the brain track set) versus a random set.\n",
    "\n",
    "When I refer to window size, this is the distance around a significant snp that is eliminated in the GWAS 1 procedure. SNPs are selected by order of p-value, with any snps with base pair locations within the window size (to the left or right) being eliminated (on the basis that there would be LD etc.). This repeats until there are no SNPs remaining that meet the p-value threshold. In Dr. Cai's original study, she used +/- 1Mb as the window size, and we have replicated this procedure. However, this creates very large windows and potentially doesn't differentiate our method as well (to be seen in this notebook).\n",
    "\n",
    "In the gwas_1_single_track_analysis directory, I performed multiple analyses comparing the results of our method using brain track sets versus random sets, using a 1Mb window size. Suprisingly, while the brain tracks were a bit better, they weren't as improved as we'd expect. On closer inspection, within the matching loci, the distance between our identified SNP and the study's identified SNP was at most 500kB, while the same was not necessarily true for the random tracks. As such, it may be that the 1Mb window is too large to differentiate our method - or, it could be that tissue specificity is not as important as we assumed.\n",
    "\n",
    "In this notebook, I will provide the results of running our method with various window sizes (250kb, 375kb, 500kb, 750kb, 1Mb), comparing the brain track set (\"reference\") to the aggregated result of 16 random sets (id = a through p). The random sets contain tracks that are not brain tracks, and were sampled without replacement, so that the sets contain an diverse representation of all the tracks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "window_sizes = [250000, 375000, 500000, 750000, 1000000]\n",
    "\n",
    "keys = ['a', 'b', 'c', 'd', 'e', 'f','g','h','i','j','k','l','m','n','o','p']\n",
    "\n",
    "coding_snps_file =  pd.read_csv('../gwas_1_single_track_analysis/coding_region_set.csv')\n",
    "coding_snps_set = set(coding_snps_file['snp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window</th>\n",
       "      <th>total_snps</th>\n",
       "      <th>total_coding</th>\n",
       "      <th>total_non_coding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000000</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>750000</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500000</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>375000</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250000</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    window total_snps total_coding total_non_coding\n",
       "4  1000000         29            0               29\n",
       "3   750000         31            0               31\n",
       "2   500000         37            1               36\n",
       "1   375000         37            0               37\n",
       "0   250000         44            0               44"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_results = pd.DataFrame(columns = ['window', 'total_snps', 'total_coding', 'total_non_coding'])\n",
    "# Create a reference table\n",
    "for window in window_sizes: \n",
    "    reference_directory = f\"snp_lists_results/id=reference/window={window}/filtered_snps_gwas_1_sd=0.0.csv\"\n",
    "    reference_snps = pd.read_csv(reference_directory)\n",
    "    reference_coding_snps= len(reference_snps[reference_snps['snp'].isin(coding_snps_set)])\n",
    "    reference_results = reference_results._append({'window': window, 'total_snps': len(reference_snps), \n",
    "                        'total_coding':reference_coding_snps, 'total_non_coding': (len(reference_snps)-reference_coding_snps)}, ignore_index=True)\n",
    "    \n",
    "reference_results.sort_values(by=['window'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table simply outlines the reference set sizes for each window size. For example, for window size 250000, the reference set contains 44 total snps, 0 of which are coding.\n",
    "Below, I will create a table for each window size. Note that all of the results are done using a 1.0 SD threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to create a table for each window size\n",
    "def create_window_size_results(window):\n",
    "    # Initialize the main DataFrame\n",
    "    df = pd.DataFrame(columns=[\"set\", \"total_snps_found\", \"total_snps_overlap\", \"total_loci_overlap\", \"total_coding_snps\", \"average_p_value\"])\n",
    "\n",
    "    # Create the reference result\n",
    "    reference_data = pd.read_csv(f\"combined_results/id=reference/window={window}/combined_result_sd=0.5.csv\").iloc[0]\n",
    "    reference_dict = {\n",
    "        \"set\": \"reference\",\n",
    "        \"total_snps_found\": reference_data['num_snps_found'],\n",
    "        \"total_snps_overlap\": reference_data['num_snps_overlap'],\n",
    "        \"total_loci_overlap\": reference_data['num_loci_overlap'],\n",
    "        \"total_coding_snps\": reference_data['num_coding_snps'],\n",
    "        \"average_p_value\": reference_data['p_value']\n",
    "    }\n",
    "    df = pd.concat([df, pd.DataFrame([reference_dict])], ignore_index=True)\n",
    "\n",
    "    # Initialize a DataFrame to collect all random set data\n",
    "    id_data = pd.DataFrame(columns=[\"set\", \"total_snps_found\", \"total_snps_overlap\", \"total_loci_overlap\", \"total_coding_snps\", \"average_p_value\"])\n",
    "    keys = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p']\n",
    "\n",
    "    # Collect data for each random set (a - p)\n",
    "    for key in keys:\n",
    "        id_data_path = f\"combined_results/id={key}/window={window}/combined_result_sd=0.5.csv\"\n",
    "        id_data_df = pd.read_csv(id_data_path).iloc[0]\n",
    "        id_data_dict = {\n",
    "            \"set\": key,\n",
    "            \"total_snps_found\": id_data_df['num_snps_found'],\n",
    "            \"total_snps_overlap\": id_data_df['num_snps_overlap'],\n",
    "            \"total_loci_overlap\": id_data_df['num_loci_overlap'],\n",
    "            \"total_coding_snps\": id_data_df['num_coding_snps'],\n",
    "            \"average_p_value\": id_data_df['p_value']\n",
    "        }\n",
    "        id_data = pd.concat([id_data, pd.DataFrame([id_data_dict])], ignore_index=True)\n",
    "\n",
    "    # Aggregate the statistics across all random sets\n",
    "    agg_id_data_mean = {\n",
    "        \"set\": \"random_aggregated_mean\",\n",
    "        \"total_snps_found\": id_data['total_snps_found'].mean(),\n",
    "        \"total_snps_overlap\": id_data['total_snps_overlap'].mean(),\n",
    "        \"total_loci_overlap\": id_data['total_loci_overlap'].mean(),\n",
    "        \"total_coding_snps\": id_data['total_coding_snps'].mean(),\n",
    "        \"average_p_value\": id_data['average_p_value'].mean()\n",
    "    }\n",
    "    agg_id_data_median = {\n",
    "        \"set\": \"random_aggregated_median\",\n",
    "        \"total_snps_found\": id_data['total_snps_found'].median(),\n",
    "        \"total_snps_overlap\": id_data['total_snps_overlap'].median(),\n",
    "        \"total_loci_overlap\": id_data['total_loci_overlap'].median(),\n",
    "        \"total_coding_snps\": id_data['total_coding_snps'].median(),\n",
    "        \"average_p_value\": id_data['average_p_value'].median()\n",
    "    }\n",
    "    agg_id_data_max = {\n",
    "        \"set\": \"random_aggregated_max\",\n",
    "        \"total_snps_found\": id_data['total_snps_found'].max(),\n",
    "        \"total_snps_overlap\": id_data['total_snps_overlap'].max(),\n",
    "        \"total_loci_overlap\": id_data['total_loci_overlap'].max(),\n",
    "        \"total_coding_snps\": id_data['total_coding_snps'].max(),\n",
    "        \"average_p_value\": id_data['average_p_value'].max()\n",
    "    }\n",
    "    agg_id_data_min = {\n",
    "        \"set\": \"random_aggregated_min\",\n",
    "        \"total_snps_found\": id_data['total_snps_found'].min().astype(int),\n",
    "        \"total_snps_overlap\": id_data['total_snps_overlap'].min(),\n",
    "        \"total_loci_overlap\": id_data['total_loci_overlap'].min(),\n",
    "        \"total_coding_snps\": id_data['total_coding_snps'].min(),\n",
    "        \"average_p_value\": id_data['average_p_value'].min()\n",
    "    }\n",
    "\n",
    "    # Add the aggregated results to the main DataFrame\n",
    "    df = pd.concat([df, pd.DataFrame([agg_id_data_mean])], ignore_index=True)\n",
    "    df = pd.concat([df, pd.DataFrame([agg_id_data_median])], ignore_index=True)\n",
    "    df = pd.concat([df, pd.DataFrame([agg_id_data_max])], ignore_index=True)\n",
    "    df = pd.concat([df, pd.DataFrame([agg_id_data_min])], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result for window size = 1Mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hf/9rstklcn237dv4bv1x1d6x740000gn/T/ipykernel_48662/2637321685.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([reference_dict])], ignore_index=True)\n",
      "/var/folders/hf/9rstklcn237dv4bv1x1d6x740000gn/T/ipykernel_48662/2637321685.py:36: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  id_data = pd.concat([id_data, pd.DataFrame([id_data_dict])], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set</th>\n",
       "      <th>total_snps_found</th>\n",
       "      <th>total_snps_overlap</th>\n",
       "      <th>total_loci_overlap</th>\n",
       "      <th>total_coding_snps</th>\n",
       "      <th>average_p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reference</td>\n",
       "      <td>32.0000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>9.215529e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>random_aggregated_mean</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>17.375</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.4375</td>\n",
       "      <td>8.108919e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_aggregated_median</td>\n",
       "      <td>31.0000</td>\n",
       "      <td>17.500</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.5000</td>\n",
       "      <td>8.156106e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>random_aggregated_max</td>\n",
       "      <td>35.0000</td>\n",
       "      <td>21.000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>9.725933e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random_aggregated_min</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>14.000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>6.988058e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        set  total_snps_found  total_snps_overlap  \\\n",
       "0                 reference           32.0000              14.000   \n",
       "1    random_aggregated_mean           31.4375              17.375   \n",
       "2  random_aggregated_median           31.0000              17.500   \n",
       "3     random_aggregated_max           35.0000              21.000   \n",
       "4     random_aggregated_min           30.0000              14.000   \n",
       "\n",
       "   total_loci_overlap  total_coding_snps  average_p_value  \n",
       "0                28.0             3.0000     9.215529e-08  \n",
       "1                28.0             2.4375     8.108919e-08  \n",
       "2                28.0             2.5000     8.156106e-08  \n",
       "3                28.0             3.0000     9.725933e-08  \n",
       "4                28.0             1.0000     6.988058e-08  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_window_size_results(1000000)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total possible snps / overlap loci was 29. They all recover 28 tracks. This is not a suprise as this has been seen in already in the gwas_1_single_track_analysis notebooks.  In addition, we see that the number of matching snps in the random sets, on average is better than the reference set. We also see a lower number of snps found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result for window size = 750000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hf/9rstklcn237dv4bv1x1d6x740000gn/T/ipykernel_48662/2637321685.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([reference_dict])], ignore_index=True)\n",
      "/var/folders/hf/9rstklcn237dv4bv1x1d6x740000gn/T/ipykernel_48662/2637321685.py:36: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  id_data = pd.concat([id_data, pd.DataFrame([id_data_dict])], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set</th>\n",
       "      <th>total_snps_found</th>\n",
       "      <th>total_snps_overlap</th>\n",
       "      <th>total_loci_overlap</th>\n",
       "      <th>total_coding_snps</th>\n",
       "      <th>average_p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reference</td>\n",
       "      <td>34.0000</td>\n",
       "      <td>14.0000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>9.215529e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>random_aggregated_mean</td>\n",
       "      <td>33.4375</td>\n",
       "      <td>18.3125</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.8125</td>\n",
       "      <td>8.108919e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_aggregated_median</td>\n",
       "      <td>33.0000</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>8.156106e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>random_aggregated_max</td>\n",
       "      <td>37.0000</td>\n",
       "      <td>23.0000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>9.725933e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random_aggregated_min</td>\n",
       "      <td>32.0000</td>\n",
       "      <td>14.0000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>6.988058e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        set  total_snps_found  total_snps_overlap  \\\n",
       "0                 reference           34.0000             14.0000   \n",
       "1    random_aggregated_mean           33.4375             18.3125   \n",
       "2  random_aggregated_median           33.0000             18.0000   \n",
       "3     random_aggregated_max           37.0000             23.0000   \n",
       "4     random_aggregated_min           32.0000             14.0000   \n",
       "\n",
       "   total_loci_overlap  total_coding_snps  average_p_value  \n",
       "0                30.0             3.0000     9.215529e-08  \n",
       "1                30.0             1.8125     8.108919e-08  \n",
       "2                30.0             2.0000     8.156106e-08  \n",
       "3                30.0             3.0000     9.725933e-08  \n",
       "4                30.0             1.0000     6.988058e-08  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_window_size_results(750000)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see that there is not much difference between the brain track set and the random set. The reference value here is 31 and we have the brain tracks recovering 30 snps and the random sets recovering 30. The number of matching snps is higher on average in the random set, which is again suprising. The 3 coding snps found in the 1Mb analysis are also seen consistently here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result for window size = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hf/9rstklcn237dv4bv1x1d6x740000gn/T/ipykernel_48662/2637321685.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([reference_dict])], ignore_index=True)\n",
      "/var/folders/hf/9rstklcn237dv4bv1x1d6x740000gn/T/ipykernel_48662/2637321685.py:36: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  id_data = pd.concat([id_data, pd.DataFrame([id_data_dict])], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set</th>\n",
       "      <th>total_snps_found</th>\n",
       "      <th>total_snps_overlap</th>\n",
       "      <th>total_loci_overlap</th>\n",
       "      <th>total_coding_snps</th>\n",
       "      <th>average_p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reference</td>\n",
       "      <td>40.0000</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.000</td>\n",
       "      <td>9.215529e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>random_aggregated_mean</td>\n",
       "      <td>39.4375</td>\n",
       "      <td>24.3125</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2.625</td>\n",
       "      <td>8.108919e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_aggregated_median</td>\n",
       "      <td>39.0000</td>\n",
       "      <td>24.5000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.000</td>\n",
       "      <td>8.156106e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>random_aggregated_max</td>\n",
       "      <td>43.0000</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.000</td>\n",
       "      <td>9.725933e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random_aggregated_min</td>\n",
       "      <td>38.0000</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2.000</td>\n",
       "      <td>6.988058e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        set  total_snps_found  total_snps_overlap  \\\n",
       "0                 reference           40.0000             20.0000   \n",
       "1    random_aggregated_mean           39.4375             24.3125   \n",
       "2  random_aggregated_median           39.0000             24.5000   \n",
       "3     random_aggregated_max           43.0000             29.0000   \n",
       "4     random_aggregated_min           38.0000             20.0000   \n",
       "\n",
       "   total_loci_overlap  total_coding_snps  average_p_value  \n",
       "0                36.0              3.000     9.215529e-08  \n",
       "1                36.0              2.625     8.108919e-08  \n",
       "2                36.0              3.000     8.156106e-08  \n",
       "3                36.0              3.000     9.725933e-08  \n",
       "4                36.0              2.000     6.988058e-08  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_window_size_results(500000)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar results to above, the reference value is n = 37 with 1 coding snp. \n",
    "### Result for window size = 375000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hf/9rstklcn237dv4bv1x1d6x740000gn/T/ipykernel_48662/2637321685.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([reference_dict])], ignore_index=True)\n",
      "/var/folders/hf/9rstklcn237dv4bv1x1d6x740000gn/T/ipykernel_48662/2637321685.py:36: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  id_data = pd.concat([id_data, pd.DataFrame([id_data_dict])], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set</th>\n",
       "      <th>total_snps_found</th>\n",
       "      <th>total_snps_overlap</th>\n",
       "      <th>total_loci_overlap</th>\n",
       "      <th>total_coding_snps</th>\n",
       "      <th>average_p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reference</td>\n",
       "      <td>40.0000</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>9.215529e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>random_aggregated_mean</td>\n",
       "      <td>39.4375</td>\n",
       "      <td>23.5625</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.9375</td>\n",
       "      <td>8.108919e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_aggregated_median</td>\n",
       "      <td>39.0000</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>8.156106e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>random_aggregated_max</td>\n",
       "      <td>43.0000</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>9.725933e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random_aggregated_min</td>\n",
       "      <td>38.0000</td>\n",
       "      <td>19.0000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>6.988058e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        set  total_snps_found  total_snps_overlap  \\\n",
       "0                 reference           40.0000             20.0000   \n",
       "1    random_aggregated_mean           39.4375             23.5625   \n",
       "2  random_aggregated_median           39.0000             24.0000   \n",
       "3     random_aggregated_max           43.0000             29.0000   \n",
       "4     random_aggregated_min           38.0000             19.0000   \n",
       "\n",
       "   total_loci_overlap  total_coding_snps  average_p_value  \n",
       "0                36.0             2.0000     9.215529e-08  \n",
       "1                36.0             1.9375     8.108919e-08  \n",
       "2                36.0             2.0000     8.156106e-08  \n",
       "3                36.0             3.0000     9.725933e-08  \n",
       "4                36.0             1.0000     6.988058e-08  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_window_size_results(375000)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar results to above, the reference value is n = 37 with 0 coding snps. \n",
    "### Result for window size = 250000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hf/9rstklcn237dv4bv1x1d6x740000gn/T/ipykernel_48662/2637321685.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([reference_dict])], ignore_index=True)\n",
      "/var/folders/hf/9rstklcn237dv4bv1x1d6x740000gn/T/ipykernel_48662/2637321685.py:36: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  id_data = pd.concat([id_data, pd.DataFrame([id_data_dict])], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set</th>\n",
       "      <th>total_snps_found</th>\n",
       "      <th>total_snps_overlap</th>\n",
       "      <th>total_loci_overlap</th>\n",
       "      <th>total_coding_snps</th>\n",
       "      <th>average_p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reference</td>\n",
       "      <td>47.000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>9.215529e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>random_aggregated_mean</td>\n",
       "      <td>46.625</td>\n",
       "      <td>25.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.9375</td>\n",
       "      <td>8.108919e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_aggregated_median</td>\n",
       "      <td>46.500</td>\n",
       "      <td>25.5</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>8.156106e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>random_aggregated_max</td>\n",
       "      <td>50.000</td>\n",
       "      <td>32.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>9.725933e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random_aggregated_min</td>\n",
       "      <td>45.000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>6.988058e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        set  total_snps_found  total_snps_overlap  \\\n",
       "0                 reference            47.000                21.0   \n",
       "1    random_aggregated_mean            46.625                25.0   \n",
       "2  random_aggregated_median            46.500                25.5   \n",
       "3     random_aggregated_max            50.000                32.0   \n",
       "4     random_aggregated_min            45.000                19.0   \n",
       "\n",
       "   total_loci_overlap  total_coding_snps  average_p_value  \n",
       "0                43.0             2.0000     9.215529e-08  \n",
       "1                43.0             1.9375     8.108919e-08  \n",
       "2                43.0             2.0000     8.156106e-08  \n",
       "3                43.0             3.0000     9.725933e-08  \n",
       "4                43.0             1.0000     6.988058e-08  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_window_size_results(250000)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar results to above, the reference value is n = 44. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial impressions: it may be that SAD scores >= 0.5 SD are just really good at capturing high activity / important genomic regions. It may be very hard to differentiate the methods at this level, because no matter what the track set, we are re-capturing essentially every SNP. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis has been repeated in 1.0 SD and 2.0 SD, achieving similar results. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
